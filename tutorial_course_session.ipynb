{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infotheory Surprisal from Language Models Tutorial\n",
    "\n",
    "In this tutorial we will reproduce a part of the experiments in [1] on the Natural Stories SPR (self-paced reading) corpus [2]. \n",
    "We will calculate surprisal values with different sizes of GPT2. The model (sizes) are provided by Hugging Face Transformers, and are pretrained on large-scale English text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Surprisal Toolkit Warmup\n",
    "\n",
    "As a warm-up to the tutorial, let's familiarize with calculating surprisal over tokens using an interactive toolkit. We will revisit this toolkit later to quickly retrieve results.\n",
    "\n",
    "First, visit the Surprisal Toolkit website: [our-university-server-URL-here].\n",
    "\n",
    "The Surprisal Toolkit allows you to enter text or upload .txt files in order to calculate surprisal scores, using probabilities processed by several large language models available from Hugging Face (https://huggingface.co/models).\n",
    "\n",
    "#### Step 1: Process a sentence or two in the text box.\n",
    "\n",
    "- Choose any model and click `Compute Surprisal`. Note that the context for calculating surprisal is limited to each line of text.\n",
    "- *If you choose a gpt2 model, you will need to also check \"Prepend token\".*\n",
    "\n",
    "#### Step 2: View the results preview. \n",
    "- Scroll through the per-token results in the `Results` tab. \n",
    "\n",
    "- *What do you notice about the tokens compared to the words in the original sentence(s)? What do you notice about the surprisal values across tokens? Between tokens of the same word? At the end of a sentence? At the start of the next sentence?*\n",
    "\n",
    "- Click `Download Results` to save the calculations.\n",
    "\n",
    "#### Step 3: View plotted results.\n",
    "- Visit the `Plots` tab to view a plot of tokens by surprisal, per line of text. Select a sentence to view its plot.\n",
    "\n",
    "- Hover over the plot to view a row of icons. <img src=\"media/plot-icons-screenshot.png\" width=\"200\"> These allow you to navigate the plot by zooming in and out, and panning across the axes.\n",
    "\n",
    "    - Zoom into a small section of the plotted sentence by clicking the `Zoom` icon and dragging a box over your preferred selection. \n",
    "    - Click `Pan` to move across the sentence.\n",
    "\n",
    "- *How does surprisal change across tokens in the sentence? Which token transitions have the highest surprisal scores? Which transitions show a decrease in surprisal? Do the model estimates appear reasonable based on your own expectations?*\n",
    "    \n",
    "- Save the plot by clicking the `Download plot as a png` :camera: icon.\n",
    "\n",
    "#### Step 4: Repeat and compare plots.\n",
    "- Repeat the surprisal computation with the **same input** and a **different, larger or smaller model** from the list. Download the results.tsv file and the plot png. \n",
    "- What do you notice about the results between models?\n",
    "\n",
    "#### Step further: exploration.\n",
    "If you'd like to explore further:\n",
    "- Notice the option to check `Prepend token`. This is relevant for the gpt2 models based on how they process text. What happens to the results when you leave it unchecked?\n",
    "- How consistent are these models? Try entering the same sentence more than once into the textbox.\n",
    "- Can you predict a very surprising or unsurprising sentence, grammatical or not, according to a given model? Find 2 sentences to compare and plot to illustrate your hypothesized prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install required packages\n",
    "\n",
    "We assume that you are on a Linux machine, or use WSL if you are on Windows. \n",
    "The recommended way to run this tutorial is with a virtual Python environment on your local machine, but you can also use [colab](https://colab.research.google.com/), which already has most of the dependencies installed, and comes with optional GPU-acceleration.\n",
    "\n",
    "If you are not on colab, you will have to install some dependencies first. In colab you only need to install the ```lmerTest``` and the Python packages other than PyTorch (see below).\n",
    "\n",
    "- Install the appropriate PyTorch version from [here](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use the ```lmerTest``` package to fit a linear mixed effect model (LME) to predict self-paced reading times from the surprisal values derived from the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!R -e 'oo <- options(repos = \"https://cran.r-project.org/\"); install.packages(\"Matrix\"); install.packages(\"lme4\"); install.packages(\"lmerTest\"); options(oo)'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```pymer4``` is a Python wrapper around the ```lmerTest``` package, and saves us from messing with R code in a notebook.\n",
    "- ```transformers``` is the Python library we use to load and run the pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers pymer4 scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: if you are running Jupyter Notebook locally, and you find your computer short on memory, uncomment and try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove all variables from memory, except for those defined in a configuration file\n",
    "# %reset -f\n",
    "\n",
    "# # Collect and free memory no longer in use by the notebook\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Natural Stories SPR data\n",
    "\n",
    "Let's download the corpus from Github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/languageMIT/naturalstories.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data is in a file ```all_stories.tok```, with each word assigned to a story ('item') and position ('zone'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "natural_stories_path = os.path.join(\"naturalstories/naturalstories_RTS\", \"all_stories.tok\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data in a DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stories = []\n",
    "df = pd.read_csv(natural_stories_path, sep='\\t', header=0)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all stories in a single text file with one story per line. These will be used to get surprisal scores for each word in the corpus, via the Surprisal Toolkit. Putting each story in one line tells the toolkit that it should maximize the model's context window, e.g. 1024 tokens for the smallest GPT2 model. The tokens of a story that don't fit into a context window will be placed at the beginning of a new context window, and therefore be conditioned on a smaller number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"naturalstories.txt\", \"w\") as f:\n",
    "    for item in df.item.unique():\n",
    "        df_item = df[df.item == item]\n",
    "        text = \" \".join(df_item.word)\n",
    "        f.write(text + (\"\\n\" if item < 10 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the reading time data in `naturalstories/naturalstories_RTS/processed_RTS.tsv` into a Dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_rts_path = os.path.join(\"naturalstories/naturalstories_RTS\", \"processed_RTs.tsv\")\n",
    "df_rts = pd.read_csv(subject_rts_path, sep=\"\\t\")\n",
    "df_rts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reading time dataframe has several columns that are relevant for the reading time prediction task: `word` (the form of the word as it was presented to the participant of the reading time study), `item` (the id of the story), `RT` (the raw reading time in milliseconds) and `nItem` (the frequency if the word in the Natural Stories corpus). While `RT` is the response variable we are interested in, `item`, `word`, and `nItem` will be used as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WorkerId` is the id of the participant. As we are not interested in *individual* variability, we average the `RT` column over all `WorkerIds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rts = df_rts.drop(columns=[\"WorkerId\", \"WorkTimeInSeconds\", \"correct\"]). \\\n",
    "    groupby(by=[\"item\", \"zone\", \"word\", \"nItem\"]). \\\n",
    "    mean(). \\\n",
    "    reset_index()\n",
    "\n",
    "# Drop word column\n",
    "df_rts = df_rts.drop([\"word\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Surprisal from GPT2 via LM Toolkit\n",
    "\n",
    "Instead of calculating surprisal by hand, we will instead use the functionality of the LM Toolkit to get surprisal on Natural Stories from 3 sizes of GPT2: `gpt2` (125m parameters, the small or base model, listed as `gpt2-small`), `gpt2-medium` (350m parameters) and `gpt2-large` (774m parameters). \n",
    "\n",
    "1. Upload the text file `naturalstories.txt` created above via the `Choose File` button, select `gpt2` as the model, check the `Prepend Token` box and calculate surprisal.\n",
    "2. Download the `results.tsv` file and rename it to `results_gpt2_small.tsv`\n",
    "3. Repeat for the other sizes of GPT2\n",
    "\n",
    "The resulting file should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sentence_id\ttoken\tsurprisal\ttoken_id\n",
    "0\tIf\t7.7582244873046875\t1532\n",
    "0\tĠyou\t0.8087347149848938\t345\n",
    "0\tĠwere\t5.417769908905029\t547\n",
    "0\tĠto\t2.095001459121704\t284\n",
    "0\tĠjourney\t14.621973037719727\t7002\n",
    "0\tĠto\t2.1376800537109375\t284\n",
    "0\tĠthe\t2.6726841926574707\t262\n",
    "0\tĠNorth\t6.569890975952148\t2258\n",
    "0\tĠof\t7.013443946838379\t286\n",
    "0\tĠEngland\t2.3848724365234375\t4492\n",
    "0\t,\t2.2026619911193848\t11\n",
    "0\tĠyou\t1.6058685779571533\t345\n",
    "0\tĠwould\t1.346909761428833\t561\n",
    "0\tĠcome\t6.167995452880859\t1282\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sentence_id` column contains the id of the story, `token` the words or subtokens the story was split into by GPT2's tokenizer, and `surprisal` contains the per-token surprisal. Since we want to model reading times on the *word* level, we have to reconstruct the words in the input by merging their subtokens. Word-level surprisal is obtained by *summing* the surprisal of the subtokens belonging to a particular word, following the chain rule of conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Word-level surprisal from subtoken-level surprisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the stories scored with `gpt2` into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored = pd.read_csv(\"results_gpt2_small.tsv\", sep=\"\\t\")\n",
    "# df_scored = pd.read_csv(\"results_gpt2_medium.tsv\", sep=\"\\t\")\n",
    "# df_scored = pd.read_csv(\"results_gpt2_large.tsv\", sep=\"\\t\")\n",
    "df_scored.iloc[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that most words are not split into subtokens. The affixed \"Ġ\" indicates that the token is the first token of an orthographic word, and subwords are indicated by the absence of the \"Ġ\". This can be seen with \"Ġmo\", \"ors\", which we have to merge into \"moors\" and whose surprisal values need to be summed: $15.3574 + 9.2472 = 24.6046$. This needs to be done for all words that have been split into subwords.\n",
    "\n",
    "Finish the implementation of the function `get_word_surprisal` below, looping over all tokens in the results file, merging subtokens per word and summing their surprisals (you can test your code by running the code cell immediately below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def get_word_surprisal(tokens: List[str], token_surprisal: List[float]) -> Tuple[List[str], List[float]]:\n",
    "    \"\"\" Calculates per-word surprisal from a list of subword tokens and subword surprisal values\n",
    "        by adding up the surprisal values of the subword tokens.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : List[str]\n",
    "        The list of tokens. Each word-initial token is expected to start with the character 'Ġ',\n",
    "        which is added by the GPT2 tokenizer.\n",
    "    token_surprisal : List[float]\n",
    "        The list of surprisal values corresponding to the subword tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[float]]\n",
    "        The reassembled words and their surprisal values.\n",
    "    \"\"\"\n",
    "    word_surprisal = []\n",
    "    words = []\n",
    "\n",
    "    # TODO: your code here\n",
    "\n",
    "    return words, word_surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_story_1 = df_scored[df_scored.sentence_id == 0]\n",
    "\n",
    "words, word_surprisals = get_word_surprisal(df_story_1.token.tolist(), df_story_1.surprisal.tolist())\n",
    "\n",
    "print(words)\n",
    "print(word_surprisals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to obtain word-level surprisal for all stories! The for loop below will add 6 more columns to the dataframe: 3 columns with surprisal from the current word `surp_0` and surprisal from the two preceding words (`surp_1`, `surp_2`), which will all be used as predictors for `word_0`'s reading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_surprisals = {}\n",
    "\n",
    "for sentence_id in df_scored.sentence_id.unique():\n",
    "    \n",
    "    df_item_scored = df_scored[df_scored.sentence_id == sentence_id]\n",
    "\n",
    "    words, word_surprisals = get_word_surprisal(df_item_scored.token.tolist(), df_item_scored.surprisal.tolist())\n",
    "\n",
    "    words_0, words_1, words_2 = words[2:], words[1:-1], words[:-2]\n",
    "    surp_0, surp_1, surp_2 = word_surprisals[2:], word_surprisals[1:-1], word_surprisals[:-2]\n",
    "\n",
    "    df_story = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"word_0\": words_0, \"surp_0\": surp_0,\n",
    "            \"word_1\": words_1, \"surp_1\": surp_1,\n",
    "            \"word_2\": words_2, \"surp_2\": surp_2\n",
    "        }\n",
    "    )\n",
    "\n",
    "    words_pre = df[df[\"item\"] == sentence_id+1].word.tolist()\n",
    "    words_post = df_story.word_0.tolist()\n",
    "\n",
    "    # make sure that the number of words is identical to that in the original data\n",
    "    assert len(words_pre)-2 == len(words_post), f\"Story {item}: {len(words_pre)-2}!={len(words_post)}\"\n",
    "\n",
    "    story_surprisals[sentence_id+1] = df_story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add another baseline predictor: The length of the 3 preceding words, in characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add position in sentence & sentence id as predictor\n",
    "for item, df_story in story_surprisals.items():\n",
    "\n",
    "    df_story[\"zone\"] = range(1, len(df_story) +1)\n",
    "    df_story[\"word_0_len\"] = [len(word) for word in df_story.word_0]\n",
    "    df_story[\"word_1_len\"] = [len(word) for word in df_story.word_1]\n",
    "    df_story[\"word_2_len\"] = [len(word) for word in df_story.word_2]\n",
    "    # needed for joining\n",
    "    df_story[\"item\"] = [item for _ in range(len(df_story))]\n",
    "\n",
    "df_surp = pd.concat(story_surprisals.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the surprisal/word length predictors into a single dataframe that will serve as the input to the LMER model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictors into the new data frame\n",
    "df_lmer = pd.merge(df_surp, df_rts, on=[\"item\", \"zone\"], how=\"inner\")\n",
    "print(df_lmer.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the common steps in the reading time prediction literature to 1) log-transform word frequencies and 2) center the raw reading times around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_lmer[\"word_0_freq\"] = np.log(df_lmer.nItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lmer[\"RT\"] = (df_lmer[\"RT\"] - np.mean(df_lmer[\"RT\"]))/np.std(df_lmer[\"RT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit a baseline LME model, that is, a regression model with the baseline predictors of world length and frequency with random intercepts for story (`item`) and word (`word_0`). The baseline model doesn't include predictors of surprisal. The quality of fit of our model to the reading time data is captured in the so-called log-likelihood of the model. \n",
    "\n",
    "A lower log-likelihood (compared to the baseline model) means that adding a predictor (surprisal in our case) *increases* the quality fit of the model. In the case of surprisal, this means that having it as one of the predictors yields more accurate predictions than not having it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymer4.models import Lmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lmer(\n",
    "    \"RT ~ word_0_len \\\n",
    "        + word_1_len \\\n",
    "        + word_2_len \\\n",
    "        + word_0_freq \\\n",
    "        + (1|item) \\\n",
    "        + (1|word_0)\",\n",
    "    data=df_lmer\n",
    ")\n",
    "\n",
    "model.fit()\n",
    "\n",
    "baseline_model_loglike = model.logLike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric to evaluate the fit of our model is Mean Squared Error (MSE), which is - you guessed it -  the mean squared deviation of the predicted reading time $\\hat{y}$ from the true reading time $y$:\n",
    "\n",
    "$MSE = \\frac{1}{N} \\cdot \\sum_i^{N} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "With $N$ in our case being the number of observed reading times. This is **not** equal to the number of surprisal values we obtained from GPT-2, since each word was read by more than one subject!\n",
    "\n",
    "Define a MSE function (you can also use scikit-learn for that) and apply it to the true reading times and our predictions.\n",
    "\n",
    "Note that MSE measures the amount of error in statistical models, specifically how close a regression line is to a set of data points. We want to minimize the error when fitting a model to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(model_fits, true_RTs) -> float:\n",
    "    \"\"\"\n",
    "    Computes the mean squared error (MSE) between model fits and true reading times.\n",
    "\n",
    "    Parameters:\n",
    "    - model_fits: Predicted reading times from the model.\n",
    "    - true_RTs: True reading times.\n",
    "\n",
    "    Returns:\n",
    "    - mse: Mean squared error between model fits and true reading times.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_mse = mse(model.fits, df_lmer[\"RT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline model log-likelihood: {baseline_model_loglike}\")\n",
    "print(f\"Baseline model MSE: {baseline_model_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit a baseline model, we are ready to add predictors of surprisal. Recall the meaning of `surp_0`, `surp_1`, and `surp_2`. We can use the measures of log-likelihood and MSE to assess the model's fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = Lmer(\n",
    "    \"RT ~ surp_0 \\\n",
    "        + surp_1 \\\n",
    "        + surp_2 \\\n",
    "        + word_0_len \\\n",
    "        + word_1_len \\\n",
    "        + word_2_len \\\n",
    "        + word_0_freq \\\n",
    "        + (1|item) \\\n",
    "        + (1|word_0)\",\n",
    "    data=df_lmer\n",
    ")\n",
    "\n",
    "model.fit()\n",
    "\n",
    "surp_model_mse = mse(model.fits, df_lmer[\"RT\"])\n",
    "surp_model_loglike = model.logLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Surprisal model log-likelihood: {surp_model_loglike}\")\n",
    "print(f\"Surprisal model MSE: {surp_model_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate the delta between the baseline model log-likelihood and the surprisal model log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_loglike = surp_model_loglike - baseline_model_loglike\n",
    "print(f\"Delta log-likelihood: {delta_loglike}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note down this value and repeat the analysis for the surprisal values obtained from `gpt2-medium` and `gpt2-large`. \n",
    "\n",
    "1. Which model size yields the best fit? Why?\n",
    "\n",
    "2. Do you find the same result for MSE?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Calculate surprisal from LM outputs\n",
    "\n",
    "This section explains how you can calculate surprisal from a LM without using the LM Toolkit (although the LM toolkit performs the same computations under the hood) with Hugging Face Transformers.\n",
    "\n",
    "The Hugging Face transformers GPT2 model consists of three models: 1) The tokenizer, 2) The transformer model and 3) The language model 'head'.\n",
    "The 'head' takes the output vector of the transformer model and obtains a vector of the size of the input vocabulary.\n",
    "\n",
    "This vector is called 'logits'. As you may now, in Mathematics a 'logit' is a function that maps probabilities into the set of real numbers, i.e. $[0,1] \\rightarrow \\mathbb{R}$.\n",
    "In the context of neural networks, 'logits' is used as a label for output $o$ with length $K = |o|$ of the model before the application of a softmax function, which is implied to \"reverse\" the log transformation of the probabilities.\n",
    "\n",
    "$\\text{softmax}(o)_i = \\frac{e^{o_i}}{\\sum_{j=1}^{K}e^{o_j}}$\n",
    "\n",
    "Loading GPT2 with its language modelling head and tokenizer is quite simple. The code below might run for some time, since the model is downloaded from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do that we should check if PyTorch is available and whether we're on a GPU or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the GPT2 tokenizer doesn't have a pad token, we use the eos token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to obtain the token ids to index the tensor holding the log probabilities!\n",
    "GPT-2 does not have a start of sequence symbol. We can emulate this by appending the end of sequence symbol to the tokenized sequences (this is what `prepend token` in the LM Toolkit web interface does). This is however not a true start of sequence symbol, and adding one would require retraining the embedding space. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 can take inputs of up to a size of 1024 tokens. This means that we have to break down each story into chunks of size $1024$. We do this by first tokenizing each story (resulting in subword sequences $> 1024$ tokens), and then rearranging it to fit into the model's context window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 1024\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "for item in df[\"item\"].unique():\n",
    "    df_s = df[df.item==item]\n",
    "    text = df_s.word.tolist()\n",
    "    tokenized = tokenizer(\" \".join(text))\n",
    "    # split into chunks of size context_size - 1 & prepend eos token\n",
    "    batch_reshaped = {\n",
    "        k: [[tokenizer.eos_token_id if k == \"input_ids\" else 1] + v[i:i+context_size-1] for i in range(0,2*(context_size-1),context_size-1)] for k, v in tokenized.items()\n",
    "    }\n",
    "\n",
    "    # pad last sequence\n",
    "    to_pad = context_size - len(batch_reshaped[\"input_ids\"][-1])\n",
    "    batch_reshaped[\"input_ids\"][-1] = batch_reshaped[\"input_ids\"][-1] + [tokenizer.pad_token_id for _ in range(to_pad)]\n",
    "    batch_reshaped[\"attention_mask\"][-1] = batch_reshaped[\"attention_mask\"][-1] + [0 for _ in range(to_pad)]\n",
    "    # copy labels from input ids\n",
    "    batch_reshaped[\"labels\"] = [input_id if input_id != tokenizer.pad_token_id else -100 for input_id in batch_reshaped[\"input_ids\"]]\n",
    "    inputs[item] = batch_reshaped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits attribute of the ```outputs``` object contains the output of the language modelling head. Let's define a function that applies softmax to the logits and convert the probabilities back to logarithmic scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_logprobs(logits: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" Calculates perplexity from a list of log probabilities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : Tensor of shape (sequence_length, vocab_size)\n",
    "        The tensor holding the language model outputs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        The log-probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: apply softmax to logits, take the negative log with base 2 (to obtain surprisal)\n",
    "\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block runs GPT2 on the batches we constructed earlier, applies the above function to the outputs of the LM head (the logits) and retrieves the token surprisals by indexing the tensor that holds the transformed logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "stories_scored = {}\n",
    "\n",
    "for item, batch in tqdm(inputs.items()):\n",
    "\n",
    "    batch = {k: torch.LongTensor(v).to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad(): # disable gradient computation\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    token_surprisal = get_logprobs(outputs.logits)\n",
    "\n",
    "    # we ignore the last surprisal value of each context window\n",
    "    token_surprisal = token_surprisal[:, :-1].numpy().reshape(2*(context_size-1), tokenizer.vocab_size)\n",
    "\n",
    "    # we ignore the first label of each context window\n",
    "    output_ids = batch[\"labels\"][:, 1:].numpy().squeeze().reshape(2*(context_size-1))\n",
    "\n",
    "    # index for first dimension of surprisals\n",
    "    index = torch.arange(0, output_ids.shape[0])\n",
    "    token_surprisal = token_surprisal[index, output_ids]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(output_ids)\n",
    "\n",
    "    assert len(tokens) == len(token_surprisal)\n",
    "\n",
    "    stories_scored.update({item: (tokens, token_surprisal)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-level surprisal can be calculated similarly to 2., with minor adjustments to the `get_word_surprisal` function. `token_surprisal` and `output_ids` still contain padding tokens and their surprisal values respectively. Adapt `get_word_surprisal` to ignore padding tokens and surprisal values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_surprisal(tokens: List[str], token_surprisal: List[float], pad_token: str) -> Tuple[List[str], List[float]]:\n",
    "    \"\"\" Calculates per-word surprisal from a list of subword tokens and subword surprisal values\n",
    "        by adding up the surprisal values of the subword tokens. Ignores padding tokens and their surprisal values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : List[str]\n",
    "        The list of tokens. Each word-initial token is expected to start with the character 'Ġ',\n",
    "        which is added by the GPT2 tokenizer.\n",
    "    token_surprisal : List[float]\n",
    "        The list of surprisal values corresponding to the subword tokens.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[float]]\n",
    "        The reassembled words and their surprisal values.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_surprisal = []\n",
    "    words = []\n",
    "\n",
    "    # TODO: Adapt your `get_word_surprisal` function from earlier to ignore padding tokens and surprisal values\n",
    "\n",
    "    return words, word_surprisal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure how \"surprised\" GPT2 was by each of the stories (i.e. how well it could predict each word given the previous words), we calculate its so-called *perplexity* or the *branching factor* of the model. Perplexity is closely related to surprisal, and can be calculated as the exponential of the average surprisal on a sequence $w$:\n",
    "$$\n",
    "PP(w) = 2^{\\frac{\\sum_i^{|w|} -\\log_2(w_i|w_{<i})}{|w|}}\n",
    "$$\n",
    "\n",
    "To evaluate the performance of GPT-2 on the corpus, complete the body of the perplexity function below and run it on the word surprisal values from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perplexity(surprisal: np.array) -> float:\n",
    "    \"\"\" Calculates perplexity from a list of log probabilities\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logprobs : List[float]\n",
    "        The log probabilities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The perplexity\n",
    "    \"\"\"\n",
    "    raise NotImplementedError # TODO: return the calculated perplexity and remove this error after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, (tokens, token_surprisals) in stories_scored.items():\n",
    "    # step 1: get word surprisal\n",
    "    words, word_surprisals = get_word_surprisal(tokens, token_surprisals, pad_token=tokenizer.pad_token)\n",
    "    # step 2: calculate perplexity\n",
    "    pp = perplexity(word_surprisals)\n",
    "    print(item, np.round(pp,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are feeling very adventurous you can fit baseline and surprisal LMER models for each story separately, and then plot perplexity vs. the log-likelihood delta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Oh, Byung-Doh, and William Schuler. “Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,” 2022. https://doi.org/10.48550/ARXIV.2212.12131.\n",
    "\n",
    "[2] Futrell, Richard, Edward Gibson, Harry J. Tily, Idan Blank, Anastasia Vishnevetsky, Steven T. Piantadosi, and Evelina Fedorenko. “The Natural Stories Corpus: A Reading-Time Corpus of English Texts Containing Rare Syntactic Constructions.” Language Resources and Evaluation 55, no. 1 (2021): 63–77.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infotheory-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a9209226b064ce8cf5d92eb1d547a7ce7e8253a28a3f881d3def9cd5f8eb401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
